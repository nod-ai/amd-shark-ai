# Copyright 2025 Advanced Micro Devices, Inc.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

name: CI - Boo on Mi355

on:
  workflow_dispatch:
  pull_request:
  schedule:
    # Run every 6 hour starting from 0h 30min on all days
    - cron: "30 */6 * * *"

permissions:
  contents: write

concurrency:
  # A PR number if a pull request and otherwise the commit hash. This cancels
  # queued and in-progress runs for the same PR (presubmit) or commit
  # (postsubmit). The workflow name is prepended to avoid conflicts between
  # different workflows.
  group: ${{ github.workflow }}-${{ github.event.number || github.sha }}
  cancel-in-progress: true

jobs:
  test_llama_large:
    if: ${{ github.repository_owner == 'nod-ai' || github.event_name != 'schedule' }}
    timeout-minutes: 240
    name: "Boo on Mi355"
    strategy:
      matrix:
        version: [3.12]
      fail-fast: false
    runs-on: linux-mi355-1gpu-ossci-nod-ai
    container:
      image: rocm/pytorch:rocm7.1.1_ubuntu24.04_py3.12_pytorch_release_2.9.1
      options: >
        --ipc host
        --shm-size=8G
        --cap-add=SYS_PTRACE
        --security-opt seccomp=unconfined
        --group-add video
        --device /dev/kfd
        --device /dev/dri
        #--env-file /etc/podinfo/gha-gpu-isolation-settings
    defaults:
      run:
        shell: bash
    env:
      VENV_DIR: ${{ github.workspace }}/.venv
      OFFLINE_SERVING: DISABLED
    steps:
    - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

    - name: "Setting up Python"
      id: setup_python
      uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
      with:
        python-version: ${{matrix.version}}
    - name: Create Python venv
      run: |
        #python -m venv ${VENV_DIR}
        #source ${VENV_DIR}/bin/activate
    - name: Install pip deps
      run: |
        # docker pull rocm/pytorch:rocm7.1.1_ubuntu24.04_py3.12_pytorch_release_2.9.1
        # docker run -it \
        #  --cap-add=SYS_PTRACE \
        #  --security-opt seccomp=unconfined \
        #  --device=/dev/kfd \
        #  --device=/dev/dri \
        #  --group-add video \
        #  --ipc=host \
        #  --shm-size 8G \
        #  rocm/pytorch:latest
        mkdir -p output_artifacts
        git clone https://github.com/iree-org/iree-turbine.git
        cd iree-turbine
        pip install -r pytorch-rocm-requirements.txt
        pip install -r requirements.txt -e .
        pip install -f https://iree.dev/pip-release-links.html --upgrade --pre iree-base-compiler iree-base-runtime
        pip freeze | grep 'iree' > $(pwd)/../output_artifacts/version.txt
        iree-compile --version >> $(pwd)/../output_artifacts/version.txt
        cd -
        sudo apt-get update
        sudo apt install -y gfortran build-essential binutils
        python -m pip install \
          --index-url https://d2awnip2yjpvqn.cloudfront.net/v2/gfx950-dcgpu/ \
          rocm[libraries,devel]

        # Set environment variables
        export ROCM_PATH=$(python -m rocm_sdk path --root)
        export LD_LIBRARY_PATH="$ROCM_PATH/lib":$LD_LIBRARY_PATH
        echo "ROCM_PATH=$ROCM_PATH" >> $GITHUB_ENV
        echo "LD_LIBRARY_PATH=$LD_LIBRARY_PATH" >> $GITHUB_ENV

    - name: Check runner health
      run: |
        echo "ROCM_PATH=$ROCM_PATH"
        echo "LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
        rocm-smi
        rocminfo
        iree-run-module --list_devices

    - name: Run all proxy config
      run: |
       
       git clone --filter=blob:none --no-checkout https://x-access-token:${{ secrets.AMD_SHARK_AI_GITHUB_TOKEN }}@github.com/nod-ai/amd-shark-ai-reports.git
       cd amd-shark-ai-reports
       git sparse-checkout init --cone
       git sparse-checkout set boo/all_proxy_config.txt boo/prod_conv_config.txt
       git checkout main
       cd -
       cd iree-turbine
       export ROCR_VISIBLE_DEVICES="0"
       # echo "Running all proxy config without tuning"
       # python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/all_proxy_config.txt --csv ../output_artifacts/all_proxy_run_without_tuning.csv || true
       echo "Running prod conv config without tuning"
       python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/prod_conv_config.txt --csv ../output_artifacts/prod_conv_config_without_tuning.csv || true
       # echo "Running all proxy config with tuning"
       # export BOO_TUNING_SPEC_PATH="iree/turbine/kernel/boo/runtime/tuning_specs.mlir"
       # python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/all_proxy_config.txt --csv ../output_artifacts/all_proxy_run_with_tuning.csv || true
       # export BOO_TUNING_SPEC_PATH="iree/turbine/kernel/boo/runtime/tuning_specs.mlir"
       # echo "Running prod conv config with tuning"
       # python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/prod_conv_config.txt --csv ../output_artifacts/prod_conv_config_with_tuning.csv || true
    - name: Upload log files
      if: always()
      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f
      with:
        name: llama-logs
        path: |
          output_artifacts/*.csv
          output_artifacts/version.txt
