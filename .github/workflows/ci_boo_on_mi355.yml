# Copyright 2025 Advanced Micro Devices, Inc.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

name: CI - Boo on Mi355

on:
  workflow_dispatch:
  pull_request:
  schedule:
    # Run every 6 hour starting from 0h 30min on all days
    - cron: "30 */6 * * *"

permissions:
  contents: write

concurrency:
  # A PR number if a pull request and otherwise the commit hash. This cancels
  # queued and in-progress runs for the same PR (presubmit) or commit
  # (postsubmit). The workflow name is prepended to avoid conflicts between
  # different workflows.
  group: ${{ github.workflow }}-${{ github.event.number || github.sha }}
  cancel-in-progress: true

jobs:
  test_llama_large:
    if: ${{ github.repository_owner == 'nod-ai' || github.event_name != 'schedule' }}
    timeout-minutes: 240
    name: "Release: Llama 405B FP4 Benchmarking Tests"
    strategy:
      matrix:
        version: [3.11]
      fail-fast: false
    runs-on: linux-mi355-1gpu-ossci-nod-ai
    container:
      image: 'ghcr.io/rocm/no_rocm_image_ubuntu24_04@sha256:405945a40deaff9db90b9839c0f41d4cba4a383c1a7459b28627047bf6302a26'
      options: --ipc host
        --group-add video
        --device /dev/kfd
        --device /dev/dri
        --env-file /etc/podinfo/gha-gpu-isolation-settings
      volumes:
        - /amdshark-dev:/amdshark-dev
        - /amdshark-cache:/amdshark-cache
    defaults:
      run:
        shell: bash
    env:
      VENV_DIR: ${{ github.workspace }}/.venv
      OFFLINE_SERVING: DISABLED
    steps:
    - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

    - name: "Setting up Python"
      id: setup_python
      uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
      with:
        python-version: ${{matrix.version}}
    - name: Create Python venv
      run: |
        python -m venv ${VENV_DIR}
        source ${VENV_DIR}/bin/activate
    - name: Install pip deps
      run: |
        mkdir -p output_artifacts
        git clone https://github.com/iree-org/iree-turbine.git
        cd iree-turbine
        pip install -r pytorch-rocm-requirements.txt
        pip install -r requirements.txt -e .
        pip install -f https://iree.dev/pip-release-links.html --upgrade --pre iree-base-compiler iree-base-runtime
        pip freeze | grep 'iree' > $(pwd)/../output_artifacts/version.txt
        iree-compile --version >> $(pwd)/../output_artifacts/version.txt
        cd -
    - name: Run all proxy config
      run: |
       git clone --filter=blob:none --no-checkout https://x-access-token:${{ secrets.AMD_SHARK_AI_GITHUB_TOKEN }}@github.com/nod-ai/amd-shark-ai-reports.git
       cd amd-shark-ai-reports
       git sparse-checkout init --cone
       git sparse-checkout set boo/all_proxy_config.txt boo/prod_conv_config.txt
       git checkout main
       cd -
       cd iree-turbine
       export ROCR_VISIBLE_DEVICES="0"
       #echo "Running all proxy config without tuning"
       #python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/all_proxy_config.txt --csv ../output_artifacts/all_proxy_run_without_tuning.csv || true
       echo "Running prod conv config without tuning"
       python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/prod_conv_config.txt --csv ../output_artifacts/prod_conv_config_without_tuning.csv || true
       #echo "Running all proxy config with tuning"
       #export BOO_TUNING_SPEC_PATH="iree/turbine/kernel/boo/runtime/tuning_specs.mlir"
       #python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/all_proxy_config.txt --csv ../output_artifacts/all_proxy_run_with_tuning.csv || true
       #export BOO_TUNING_SPEC_PATH="iree/turbine/kernel/boo/runtime/tuning_specs.mlir"
       #echo "Running prod conv config with tuning"
       #python iree/turbine/kernel/boo/driver/driver.py  --commands-file ../amd-shark-ai-reports/boo/prod_conv_config.txt --csv ../output_artifacts/prod_conv_config_with_tuning.csv || true
    - name: Upload log files
      if: always()
      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f
      with:
        name: llama-logs
        path: |
          output_artifacts/*.csv
          output_artifacts/version.txt

